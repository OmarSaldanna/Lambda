# file that contains info about: LLMs available, prices and context size limit.
export MODELS_FILE=lambda/models.json

# Average length of the answers. Used to estimate each prompt cost.
# Currently based on Lambda V3 logs, it was 140 tokens
export AVG_ANSWER_LEN=150

# Number of images added to the usage per credit added
export IMAGES_PER_CREDIT=2

# Encode to tokenize
# https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
export TOKENIZER_ENCODING="o200k_base"

# max number of tokens per file
# used for large mode. set to large limit - 2000
export MAX_LARGE_TOKENS=30000


############################ Error messages ########################

# thrown when a prompt isn't processed due to lack of funds
export NO_FUNDS_ERROR="Ups!, parece que te has quedado sin créditos"
# thrown when an api key isn't correct or found
export BAD_API_KEY_ERROR="API KEY no válida"
# thrown when any param is null
export BAD_REQUEST_ERROR="Request no válida"
# thrown when there are missing parameters
export MISSING_PARAMS_ERROR="Faltan parámetros"
# thrown when an error ocurred when saving a file
export FILE_SAVING_ERROR="Ups!, parece que hubo un error guardando tu archivo"
# thrown when a filename is wrong, like containing / or ..
export BAD_FILE_ERROR="Nombre de archivo no válido"
# thrown on chat where the message were not correctly processed
export CHAT_PROCESSING_ERROR="Error de procesamiento, verifica tu mensaje"