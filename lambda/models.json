{
	"__comment__": "These limits determine when on how many tokens the context is going to be reseted, they are evaluated on every prompt. These numbers can be the officials from the docs, or adjusted to less in order to reduce operation costs.",

	"limits": {
		"chat": 2000,
		"research": 4000,
		"intelligence": 2000,
		"instruct": 2000
	},
	
	"__comment__": "These are the available models. By default Lambda uses 2 from each family: GPT, Gemini and Claude. Normally including the fastest model and the smartest model.",

	"model": ["gpt-4o", "gpt-4o-mini", "claude-3-5-sonnet-20240620", "claude-3-haiku-20240307", "gemini-1.5-pro", "gemini-1.5-flash"],

	"__comment__": "Pricing costs where gotten from the oficial pages, they are Million tokens. They are used to calculate how many USDs has cost every Lambda prompt.",

	"prices": {
		"gemini-1.5-flash": [0.075, 0.30],
		"gemini-1.5-pro": [3.50, 10.50],
		
		"gpt-4o": [5, 15],
		"gpt-4o-mini": [0.15, 0.6],
		
		"claude-3-5-sonnet-20240620": [3, 15],
		"claude-3-haiku-20240307": [0.25, 1.25]
	}

}